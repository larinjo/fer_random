{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projekt R - Prepoznavanje znakova skupova podataka MNIST i Kaggle A-Z pomoću dubokih modela\n",
    "\n",
    "Kaggle - https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format\n",
    "MNIST - http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dodavanje svih potrebnih biblioteka\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import itertools\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nakon učitavanja potrebnih biblioteka, učitavamo podatke iz MNIST-a i Kaggle-a, pretvaramo ih u numpy arrayeve te spajamo MNIST train i test skupove u jedan skup jer ćemo model trenirati na Kaggle i MNIST skupovima zajedno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_tr, y_tr), (x_ts, y_ts) = tf.keras.datasets.mnist.load_data()\n",
    "csvPath = \"A_Z Handwritten Data.csv\"\n",
    "x_az = []\n",
    "y_az = []\n",
    "for row in open(csvPath):\n",
    "        row = row.split(\",\")\n",
    "        #prvi element u redu je labela slike\n",
    "        label = int(row[0])\n",
    "        #svi ostali elementi u redu su pikseli slike, pretvaramo ih u numpy array unsigned 8 bit integera (s obzirom da poprimaju vrijednosti od 0 do 255)\n",
    "        image = np.array([int(x) for x in row[1:]], dtype = \"uint8\")\n",
    "        #preoblikujem image u 28x28 matricu (slike su predstavljene kao red od 784 piksela)\n",
    "        image = image.reshape((28, 28))\n",
    "        y_az.append(label)\n",
    "        x_az.append(image)\n",
    "        \n",
    "    #pretvaranje u numpy array radi lakšeg korištenja\n",
    "y_az = np.array(y_az, dtype = \"uint8\")\n",
    "x_az = np.array(x_az, dtype = \"uint8\")\n",
    "#spajanje train i test skupa za mnist\n",
    "x_mnist = np.vstack([x_tr, x_ts])\n",
    "y_mnist = np.hstack([y_tr, y_ts])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcija za izradu podskupa podataka sa maksimalno n uzoraka po klasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subsets(data, labels, n):\n",
    "  x_samples = []\n",
    "  y_samples = []\n",
    "\n",
    "  # iteriranje po svim klasama\n",
    "  for class_ in np.unique(labels):\n",
    "    # array sa indeksima elemenata koji pripadaju klasi class_\n",
    "    class_indices = np.where(labels== class_)[0]\n",
    "    # ako ima više od n elemenata u klasi, odaberi n slučajnih elemenata\n",
    "    if len(class_indices) > n:\n",
    "      random_indices = np.random.choice(class_indices, size=n, replace=False)\n",
    "    # ako ima manje od n elemenata u klasi, odaberi sve elemente\n",
    "    else:\n",
    "      random_indices = class_indices\n",
    "    # spajanje odabranih elemenata u listu\n",
    "    x_samples.append(data[random_indices])\n",
    "    y_samples.append(labels[random_indices])\n",
    "\n",
    "  # spajanje u numpy array\n",
    "  #x_samples = np.concatenate(x_samples)\n",
    "  #y_samples = np.concatenate(y_samples)\n",
    "  x_samples = np.vstack(x_samples)\n",
    "  y_samples = np.hstack(y_samples)\n",
    "\n",
    "  # pretovrba u uint8 (0-255)\n",
    "  x_samples = np.array(x_samples, dtype=\"uint8\")\n",
    "  y_samples = np.array(y_samples, dtype=\"uint8\")\n",
    "  return (x_samples, y_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcija za izradu podskupa podataka sa maksimalno n uzoraka po klasi za skup podataka MNIST."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spajamo testni i trening MNIST skup podataka, te Kaggle skup podataka u jedan zajednički skup podataka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_datasets(x1, y1, x2, y2):\n",
    "  x = np.vstack([x1, x2])\n",
    "  y = np.hstack([y1, y2+10])\n",
    "  return (x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preuređujemo podatke tako da budu u skladu s potrebama za daljnju obradu, dakle pretvaramo vrijednosti u interval [0,1], dodajemo dimenziju kako bi bio dobili tenzor odgovarajuće dimenzije, također računamo težine klasa kako bi pri treniranju modela skup podataka bio ravnomjerniji. Te labele pretvaramo u one-hot enkodiranje. Konačno skup podataka se dijeli na trening i test skup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_prepare_split(x, y, ratio=0.2):\n",
    "\t#postavi sve vrijednosti između 0 i 1\n",
    "\tx = x / 255\n",
    "\n",
    "\t#pretvaranje u grayscale (dodavanje kanala)\n",
    "\tx = np.expand_dims(x, axis=-1)\n",
    "\n",
    "\t#shuffleanje podataka i labela zajedno sa seedom = 420 i one-hot encoding labela\n",
    "\tx, y = sklearn.utils.shuffle(x, y, random_state=420)\n",
    "\ty = tf.keras.utils.to_categorical(y, num_classes=36)\n",
    "\tprint(y.sum(axis=0))\n",
    "\n",
    "\t#izračunjavanje težina za svaku klasu\n",
    "\tclassTotals = y.sum(axis=0)\n",
    "\tclassWeight = {}\n",
    "\t# iteriranje kroz sve klase i računanje težine\n",
    "\tfor i in range(0, len(classTotals)):\n",
    "\t\tclassWeight[i] = classTotals.max() / classTotals[i]\n",
    "\n",
    "\t#podijeli na trening i test set (20% test, 80% train)\n",
    "\tX_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(x, y, test_size=ratio, random_state=420, stratify=y)\n",
    "\n",
    "\treturn (X_train, X_val, Y_train, Y_val, classWeight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konačno slijedi definicija modela, odnosno određivanje slojeva i njihovih parametara. Za model smo odabrali duboki model koji se sastoji od 2 konvolucijska sloja i 2 potpuno povezana sloja. Za aktivacijske funkcije koristimo ReLU, a za optimizaciju Adam, moglo se koristiti i SGD ili bilo koji drugi optimizator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(num_filters, kernel_size, dropout_rate, regularizers):\n",
    "    if(regularizers == 1):\n",
    "        regularizers = tf.keras.regularizers.l1(0.01)\n",
    "    elif(regularizers == 2):\n",
    "        regularizers = tf.keras.regularizers.l2(0.01)\n",
    "    elif(regularizers == 0):\n",
    "        regularizers = None\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #dodavanje konvolucijskog sloja od 32 filtra sa 3x3 kernelom, relu aktivacijskom funkcijom, input shape je 28x28x1 (jer je grayscale) i regularizacija L2\n",
    "    model.add(tf.keras.layers.Conv2D(num_filters, kernel_size, activation='relu', input_shape=(28, 28, 1)))\n",
    "    #dodavanje max pooling sloja sa 2x2 kernelom, svrha poolinga je smanjiti dimenzionalnost podataka i time smanjiti broj parametara\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    #dodavanje batch normalizacije, svrha batch normalizacije je ubrzati treniranje i smanjiti vanjske utjecaje na model, dakle oduzima se srednja vrijednost i dijeli se standardnom devijacijom\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    #dodavanje dropout sloja, svrha dropouta je sprečavanje overfittinga, odnosno slučajno isključivanje neurona (20% neurona u ovom slučaju)\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    #dodavanje konvolucijskog sloja od 64 filtra sa 3x3 kernelom, relu aktivacijskom funkcijom i regularizacija L2\n",
    "    model.add(tf.keras.layers.Conv2D(num_filters*2, kernel_size, activation='relu', kernel_regularizer=regularizers))\n",
    "     #dodavanje max pooling sloja sa 2x2 kernelom\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    #dodavanje batch normalizacije\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    #dodavanje flatten sloja, flatten sloj pretvara matricu u vektor\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    #dodavanje potpuno povezanog sloja od 64 neurona i relu aktivacijskom funkcijom\n",
    "    parameter = (num_filters*2*(((((28-(kernel_size-1))/2)-(kernel_size-1))/2)**2))\n",
    "    model.add(tf.keras.layers.Dense(parameter, activation='relu'))\n",
    "    #dodavanje potpuno povezanog sloja od 36 neurona i softmax aktivacijskom funkcijom, softmax aktivacijska funkcija vraća vjerojatnosti za svaku klasu\n",
    "    model.add(tf.keras.layers.Dense(36, activation='softmax'))\n",
    "    #kompajliranje modela, koristi se adam optimizator, categorical_crossentropy kao loss funkcija i accuracy kao metrika\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(      \n",
    "                learning_rate=0.0001,\n",
    "    ),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    #sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(optimizer=sgd,\n",
    "    #            loss='categorical_crossentropy',\n",
    "    #            metrics=['accuracy'])\n",
    "    #vraćanje modela\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kako gore nismo naveli parametre modela, parametre modela optimiziramo pomoću grid searcha. Za svaki od parametara modela određujemo raspon vrijednosti, a zatim za svaku kombinaciju parametara modela treniramo model i određujemo točnost modela na test skupu. Na kraju odabiremo parametre modela koji daju najbolju točnost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_class = 100000 #oko 60000 je najveća klasa, dakle ovo uzima cijeli dataset\n",
    "xm, ym = make_subsets(x_mnist, y_mnist, size_of_class)\n",
    "xa, ya = make_subsets(x_az, y_az, size_of_class)\n",
    "(x, y) = concat_datasets(xm, ym, xa, ya)\n",
    "(X_train, X_val, Y_train, Y_val, classWeight) = divide_prepare_split(x, y, 0.2)\n",
    "num_filters = [16, 32]\n",
    "kernel_size = [3, 5]\n",
    "dropout_rate = [0.2, 0.4]\n",
    "regularizers = [1, 2]\n",
    "param_grid = list(itertools.product(num_filters, kernel_size, dropout_rate, regularizers))\n",
    "\n",
    "#grid search\n",
    "scores = []\n",
    "for params in param_grid:\n",
    "    print(params)\n",
    "    model = create_model(params[0], params[1], params[2], params[3])\n",
    "    model.fit(X_train, Y_train, epochs=15, batch_size=128, class_weight=classWeight, validation_data=(X_val, Y_val))\n",
    "    scores.append(model.evaluate(X_train, Y_train, verbose=0))\n",
    "    \n",
    "#best_index = np.argmin([x[0] for x in scores]) #za loss\n",
    "best_index = np.argmax([x[1] for x in scores]) #za accuracy\n",
    "best_num_filters, best_kernel_size, best_dropout_rate, best_regularizers = param_grid[best_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konačno možemo istrenirati model s optimalnim parametrima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = create_model(best_num_filters, best_kernel_size, best_dropout_rate, best_regularizers)\n",
    "best_history = model.fit(X_train, Y_train, epochs=15, batch_size=128, class_weight=classWeight, validation_data=(X_val, Y_val))\n",
    "\n",
    "loss, accuracy = best_model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Test loss: \", loss)\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "best_model.save('model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nad modelom je moguće izvesti unakrsnu validaciju kako bi se odredila kvaliteta generalizacije modela. Za unakrsnu validaciju koristimo k-struku validaciju, gdje k predstavlja broj podskupova na koje se podijeli skup podataka. Za svaki od podskupova model se trenira na ostalim podskupovima, a testira se na trenutnom podskupu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 5\n",
    "kfold = sklearn.model_selection.KFold(n_splits=splits)\n",
    "#k-struka unakrsna validacija\n",
    "fold = 0\n",
    "fold_stats = []\n",
    "for train, val in kfold.split(X_train, Y_train):\n",
    "    print(fold)\n",
    "    model_cv = create_model(best_num_filters, best_kernel_size, best_dropout_rate, best_regularizers)\n",
    "    model_cv.fit(X_train[train], Y_train[train], epochs=15, batch_size=128, class_weight=classWeight, validation_data=(X_train[val], Y_train[val]))\n",
    "    fold_stats.append(model_cv.evaluate(X_train[val], Y_train[val], verbose=0))\n",
    "    fold += 1\n",
    "    \n",
    "mean_loss = np.mean([x[0] for x in fold_stats])\n",
    "std_loss = np.std([x[0] for x in fold_stats])\n",
    "mean_accuracy = np.mean([x[1] for x in fold_stats])\n",
    "std_accuracy = np.std([x[1] for x in fold_stats])\n",
    "print(\"Mean loss: \", mean_loss)\n",
    "print(\"Std loss: \", std_loss)\n",
    "print(\"Mean accuracy: \", mean_accuracy)\n",
    "print(\"Std accuracy: \", std_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluacija modela na potpunom skupu podataka (trening i test skup podataka)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spajanje mnist i az\n",
    "x_eval_conc = np.vstack([x_mnist, x_az])\n",
    "y_eval_conc = np.hstack([y_mnist, y_az+10])\n",
    "\n",
    "#prilagodba podataka u tenzore odgovarajućih dimenzija\n",
    "x_eval = np.expand_dims(x_eval_conc, axis=-1)\n",
    "y_eval = tf.keras.utils.to_categorical(y_eval_conc, num_classes=36)\n",
    "\n",
    "#evaluacija na svim podatcima\n",
    "loss, accuracy = model.evaluate(x_eval, y_eval, verbose=0)\n",
    "print(\"Eval loss: \", loss)\n",
    "print(\"Eval accuracy: \", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konačno možemo prikazati rezultate modela na trening i testnom skupu u obliku grafa loss-a i accuracy-a po epohama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = create_model(best_num_filters, best_kernel_size, best_dropout_rate, best_regularizers)\n",
    "#best_history = model.fit(X_train, Y_train, epochs=20, batch_size=128, class_weight=classWeight, validation_data=(X_val, Y_val))\n",
    "\n",
    "#Plotanje loss-a i accuracy-a za train i validation skupove\n",
    "plt.plot(best_history.history['loss'])\n",
    "plt.plot(best_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(best_history.history['accuracy'])\n",
    "plt.plot(best_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#print(best_history.history['loss'])\n",
    "#print(best_history.history['val_loss'])\n",
    "#print(best_history.history['accuracy'])\n",
    "#print(best_history.history['val_accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usporedba modela s najvišim accuracy-om i modela s najnižim loss-om u grid search-u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scores)\n",
    "#print(np.argmin([x[0] for x in scores]))\n",
    "#print(np.argmax([x[1] for x in scores]))\n",
    "\n",
    "#crtanje grafa za loss i accuracy za train i validation skupove\n",
    "plt.plot(history_list[np.argmin([x[0] for x in scores])].history['accuracy'])\n",
    "plt.plot(history_list[np.argmin([x[0] for x in scores])].history['val_accuracy'])\n",
    "plt.plot(history_list[np.argmax([x[1] for x in scores])].history['accuracy'])\n",
    "plt.plot(history_list[np.argmax([x[1] for x in scores])].history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation', 'train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_list[np.argmin([x[0] for x in scores])].history['loss'])\n",
    "plt.plot(history_list[np.argmin([x[0] for x in scores])].history['val_loss'])\n",
    "plt.plot(history_list[np.argmax([x[1] for x in scores])].history['loss'])\n",
    "plt.plot(history_list[np.argmax([x[1] for x in scores])].history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation', 'train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "336e570a73145b1c187d42459f13a8bf562fb4755ce77af72c03f327b3c3da51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
